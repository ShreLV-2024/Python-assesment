# -*- coding: utf-8 -*-
"""LVADSUSR116_Shreyas_Final_assesment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wGEP5cc3sVVl88eIUHMBLcvpi35-H3ac
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""### 1. Load dataset"""

df = pd.read_excel('Walmart_Dataset Python_Final_Assessment.xlsx')

df.head() # first 5 rows

df.columns # the different columns present

df.shape # there are a total of 3203 rows and 10 columns

df.info() # info on the datatype of each column

df.select_dtypes(include = 'object').head() # these are all the string related columns and data

df.select_dtypes(include = 'float').head() # these are all the float related columns and data

df.select_dtypes(include = 'datetime64').head() # these are the datetime columns

df.describe() # gives basic descriptive statistics about each column
# gives data about the data distribution(255, 50%, 75% quartiles)

df[df['Profit'].isnull()] # checks if a certain column has null values or not



"""### 2. Data Cleaning"""

df.head(1)

df.columns

# Checking for null values - checking with respect to each column
print(df[df['Order ID'].isna()])
print(df[df['Order Date'].isna()])
print(df[df['Ship Date'].isna()])
print(df[df['EmailID'].isna()])
print(df[df['Geography'].isna()])
print(df[df['Category'].isna()])
print(df[df['Product Name'].isna()])
print(df[df['Sales'].isna()])
print(df[df['Quantity'].isna()])
print(df[df['Profit'].isna()])

# So we can see that no columns has null values, which indicates that the dataset as a whole doesn't have null values



# Checking for duplicates
df.duplicated().unique()
# so this shows us that there exists no duplicates since the only unique value is false



"""### 3. Descriptive Statistics"""

df.select_dtypes(include = 'float').head()

df.select_dtypes(include = 'int').head()

# Desciprtive stats for sales
print(f'1. The total sales across the dataset is:',df['Sales'].sum())
print(f'2. The average sales across the dataset is:',df['Sales'].mean())
print(f'3. The median sales (central sales) across the dataset is:',df['Sales'].median())
print(f'4. The standard deviation of sales across the dataset is:',df['Sales'].std())
print(f'5. The range of sales across the dataset is:',df['Sales'].max() - df['Sales'].min())
print(f'6. The variance of sales across the dataset is:',np.sqrt(df['Sales'].std()))
print(f'7. The data distribution(25th and 75th quartile respectively) of sales across the dataset is:',df['Sales'].quantile(0.25), df['Sales'].quantile(0.75))

# Descriptive statistics for profit
print(f'1. The total profit across the dataset is:',df['Profit'].sum())
print(f'2. The average profit across the dataset is:',df['Profit'].mean())
print(f'3. The median profit (central profit value) across the dataset is:',df['Profit'].median())
print(f'4. The standard deviation of profit across the dataset is:',df['Profit'].std())
print(f'5. The range of profit across the dataset is:',df['Profit'].max() - df['Profit'].min())
print(f'6. The variance of profit across the dataset is:',np.sqrt(df['Profit'].std()))
print(f'7. The data distribution(25th and 75th quartile respectively) of profit across the dataset is:',df['Profit'].quantile(0.25), df['Profit'].quantile(0.75))

# Descriptive statistics for quantity
print(f'1. The total quantity across the dataset is:',df['Quantity'].sum())
print(f'2. The average quantity across the dataset is:',df['Quantity'].mean())
print(f'3. The median quantity (central quantity value) across the dataset is:',df['Quantity'].median())
print(f'4. The standard deviation of quantity across the dataset is:',df['Quantity'].std())
print(f'5. The range of quantity across the dataset is:',df['Quantity'].max() - df['Quantity'].min())
print(f'6. The variance of quantity across the dataset is:',np.sqrt(df['Quantity'].std()))
print(f'7. The data distribution(25th and 75th quartile respectively) of quantity across the dataset is:',df['Quantity'].quantile(0.25), df['Quantity'].quantile(0.75))



"""### 4. Data Visualization"""

df.head()

# Visualzing profit for certain geographies(10 random places)

df['Geography'].unique()

df1 = df.groupby(['Geography'])['Sales'].sum().to_frame(name = 'total profit')
df1

plt.figure(figsize = (15,8))
sns.barplot(data = df1, y = df1['total profit'][:10], x = df1.index[:10])



# We can visualizae the distribution of sales and profit by using histogram
fig,axes = plt.subplots(2,1, figsize = (10,6))

sns.histplot(data = df, x = 'Sales', bins = 10, ax = axes[0])
sns.histplot(data = df, x = 'Profit', bins = 10, ax = axes[1])



# Now lets find the count of each category using a countplot
plt.figure(figsize = (14,8))
sns.countplot(data = df, x = 'Category')



# Lets also plot a scatter plot between sales and profit and subdivide it by the Category
plt.figure(figsize = (12,7), dpi = 120)
sns.scatterplot(data = df, x = 'Sales', y = 'Profit', hue = 'Category')
plt.xlim(0,6000)



# Now lets use a pie chart to find which category had larger percetange of quantites ordered
df2 = df.groupby(['Category'])['Quantity'].sum().to_frame(name = 'Profit')
df2

plt.figure(figsize = (8,8), dpi = 150)
plt.pie(df2['Profit'], labels = df2.index, autopct = '%1.1f%%')
plt.show()





"""### 5. Identifying relationships"""

df3 = df.corr(numeric_only = True)
df3
# So here we can notice a strong postive correlation between sales and profit but a weak positive correltion between quantity and profit

sns.heatmap(data = df3, annot = True)



"""### 6. Anomaly Detection"""

# We can check outliers by using either a boxplot or quartile method

sns.boxplot(data = df, x = 'Sales')
# this visual itself provides a clue that there are many outliers

sns.boxplot(data = df, x = 'Profit')
# There are a larger number of outliers over here in profit

sns.boxplot(data = df, x = 'Quantity')



# Lets use the quartlie method on sales

Q1 = df['Sales'].quantile(0.25)
Q3 = df['Sales'].quantile(0.75)
IQR = Q3 - Q1

# The outliers are data points beyong 1.5*IQR of Q3 and below 1.5*IQR of Q1

lower_limit = Q1 - (1.5*IQR)
upper_limit = Q3 + (1.5*IQR)

df[(df['Sales'] < lower_limit) | (df['Sales'] > upper_limit)]
# So we can notice that there are 368 rows of data where there exits outlier data points in sales
# But without the business understanding, we cannot remove these outliers

# Lets use the quartlie method on profit

Q1 = df['Profit'].quantile(0.25)
Q3 = df['Profit'].quantile(0.75)
IQR = Q3 - Q1

# The outliers are data points beyong 1.5*IQR of Q3 and below 1.5*IQR of Q1

lower_limit = Q1 - (1.5*IQR)
upper_limit = Q3 + (1.5*IQR)



df[(df['Profit'] < lower_limit) | (df['Profit'] > upper_limit)]
# So we can notice that there are 465 rows of data where there exits outlier data points in profit
# But without the business understanding, we cannot remove these outliers



"""### 7. Data Discovery

#### Trend Analysis
"""

# Qn1
df1 = df.copy()

df1.head(1)
df1['Order Date'] = df1['Order Date'].astype(str)

df1['Year'] = df1['Order Date'].str.split('-').str[0]

df1['Year'] = df1['Year'].astype(int)

df5 = pd.pivot_table(data = df1, index = 'Year', values = ['Sales','Profit'], aggfunc = [np.mean, np.sum], fill_value = 0)
df5

# So we can notice that while the total sales from the year 2011 to 2012 have gone down, the mean profit in those years have gone up and
# similarly while the total profit has gone up from 2012- 2013, the mean profit has gone down

pd.options.display.max_rows = 200



# QN2
df2 =df1.groupby(['Category','Year']).aggregate({'Sales':'sum'})

df2.reset_index(inplace = True)

plt.figure(figsize = (14,7))
sns.lineplot(data = df2, x = 'Year', y = 'Sales', hue = 'Category')
plt.legend(loc = (1.05,0.5))

# So from this line chart, it is visible that Copiers have the best improvement over the years, the started lower than almost all categories but reached the peak in 2014



"""#### Customer Analysis"""

# Qn1

df1.head()

df6 = df1.groupby(['EmailID']).aggregate({'Order ID':'count','Sales':'sum'}).sort_values(by = ['Order ID','Sales'], ascending = False)

df6.head()
# these are the high orders and high sales customers
# we can notice that there are some customers who have a lot of orders but total sales is les swhich indicates they buy goods of lower prices regularly

# Qn2
category_sales = df1.groupby(['Product Name', pd.to_datetime(df['Order Date']).dt.year])['Sales'].sum().unstack()

growth_rate = category_sales.pct_change(axis=1).iloc[:, 1:]
average_growth_rate = growth_rate.mean(axis=1)

growth = average_growth_rate.idxmax()

print(f'The category that is the most sought after by the customers regularly is:',growth)



"""#### Comprehensive Analytics

1. We can find the sales velocity and order fulfillment to get an understanding on how fast or slow the orders reach the clients or customers. After collecting data, analysing the necessary regions where these indicators are being used and compare it with the average data to see which regions meet the requirement and which dont. We can use predictive analytics and forecasting to check which regions will face issues in the future with respect to order deliveries. Often quality control checks need to be done in order to ensure that the orders are not just fulfilled with respect to the time but also with respect to the quality
   
2. Factors such as the size of the market that the company is targetting and the needs of the customers are often crucial factors to understand when targetting sales across a geogprahic distribution. Demos need to be conducted across mutliple regions to see how the consumers react to the product that is supposed to be delivered in that country. Accordingly steps should be taken to market the product. The company should also see the ease of being able to handle the market by looking at the age distribution of the consumers and accordingly targetting customers based on the product.

3. High value customers can often be targetted by looking at the spending or ordering quantiteis over a time over time basis. This time could vary based on the type of product and type of service being delivered to the consumer. We can also using predictive models in order to understamd which customer has the likely ability to once again buy products. We can use customer behaviour data to understand the type of product that they have regularly bought and then target them accordingly when introducing customer loyalty and acquisition.
"""

